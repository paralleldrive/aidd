---
description: Generate human and AI agent test scripts from user journey specifications
alwaysApply: false
---
# User Testing Generator

Use UserJourney and Persona from @productmanager.mdc

Generate dual test scripts: human (think-aloud protocol, video recorded) + AI agent (executable with screenshots).

## Scripts

HumanScript:template {
  """
  # Test: ${journey.name}

  **Persona**: ${persona.name} — ${persona.role}

  ## Pre-test
  - Start screen recording
  - Clear state (cookies, cache, cart)
  - Prepare credentials if needed

  ## Steps
  For each step:
  - Goal: ${step.intent}
  - Do: ${step.action}
  - Think aloud: What do you see? Any friction?
  - Success: ${step.success}

  ## Post-test
  - Stop recording
  - What was confusing?
  - What worked well?
  - Would you complete this in real life?
  """
}

AgentScript:template {
  """
  # Agent Test: ${journey.name}

  **Environment**: Drive a real browser like a human would
  - Use built-in browser (IDE browser, Chrome, etc.)
  - Discover UI without source code access (look at page to figure out what to click)
  - Navigate and interact with actual UI as a user would
  - Click, type, scroll through real DOM elements
  - Capture screenshots from browser viewport

  **Persona behavior**:
  - Patience: ${persona.patience}/10
  - Retry: ${persona.techLevel == "expert" ? "immediate" : "exponential backoff"}
  - On failure: ${persona.patience > 5 ? "retry" : "abort"}

  ## Execution
  For each step:
  1. Interact with real UI: ${step.action}
  2. Validate rendered result: ${step.success}
  3. Screenshot browser viewport if checkpoint or failure
  4. Record feedback: difficulty, expectations vs reality, duration
  5. Retry with backoff if failed and patient

  Report: completed steps, screenshots, feedback, blockers
  """
}

generateScripts(journey) => human + agent templates with persona-mapped behavior

## Interface

/user-test <journey> - Generate human and agent scripts
/run-test <script> - Execute agent script with screenshots

Constraints {
  AI agents drive real browser like humans (no source code access, discover UI by looking)
  NOT automation frameworks (Playwright/Puppeteer require pre-knowledge of selectors)
  Agent behavior reflects persona traits (patience → retries, techLevel → strategy)
  Execution varies between runs (stochastic, not deterministic)
  Capture screenshots from real browser viewport
  Both scripts validate identical success criteria against real rendered UI
  Agents validate UI discoverability, not just technical functionality
}
